{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e68bd7-95cd-4d24-be64-76f90ecf753a",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "LangChain is a framework for developing applications powered by language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55c4b2df-5c42-4326-a586-2ad6ccce6548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.chat import SystemMessage, HumanMessagePromptTemplate\n",
    "from langchain.chains import LLMChain, ConversationChain, SimpleSequentialChain\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2d4a16-7b6e-40f8-a9d3-b4863536c7da",
   "metadata": {},
   "source": [
    "### Set up API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c09fdbed-788d-4192-863e-e99cd389006e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load OpenAI API key from env\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c26bf4-b70a-4fa6-81ee-0db916aaf758",
   "metadata": {},
   "source": [
    "### LLM\n",
    "Large Language Models (LLMs) are a core component of LangChain. LangChain does not serve its own LLMs, but rather provides a standard interface for interacting with many different LLMs.<br/>\n",
    "There are lots of LLM providers (OpenAI, Cohere, Hugging Face, etc) - the LLM class is designed to provide a standard interface for all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "242926a7-461f-41cc-bc43-1ea267e2fe0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Taco Fiesta\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0.7)\n",
    "print(llm.predict(\"Suggest a name for new mexican restaurant\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12b0a78e-d216-4e14-9260-56656fcbe416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Casa de Fuego Mexican Grill\n"
     ]
    }
   ],
   "source": [
    "print(llm.predict(\"Suggest a name for new mexican restaurant\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c3930c-8bf5-4775-8db1-d6915a8866b7",
   "metadata": {},
   "source": [
    "### Prompts\n",
    "A prompt for a language model is a set of instructions or input provided by a user to guide the model's response, helping it understand the context and generate relevant and coherent language-based output, such as answering questions, completing sentences, or engaging in a conversation.\r\n",
    "\r\n",
    "LangChain provides several classes and functions to help construct and work with prompts.\r\n",
    "\r\n",
    "1. Prompt templates: Parametrized model inputs\r\n",
    "2. Example selectors: Dynamically select examples to include in promptsompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a11823a-ca4d-48f2-b5ea-4ce472f4a09c",
   "metadata": {},
   "source": [
    "#### Prompt Templates\n",
    "Prompt templates are pre-defined recipes for generating prompts for language models.\r\n",
    "\r\n",
    "A template may include instructions, few-shot examples, and specific context and questions appropriate for a given task.\r\n",
    "\r\n",
    "LangChain provides tooling to create and work with prompt templates.\r\n",
    "\r\n",
    "LangChain strives to create model agnostic templates to make it easy to reuse existing templates across different language models.\r\n",
    "\r\n",
    "Typically, language models expect the prompt to either be a string or else a list of chat messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b1f1b46-75e7-4aff-9f55-c13e58c69db2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joke_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "joke_template.format(adjective=\"funny\", content=\"chickens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a91776bc-435c-43f7-8e2b-1edd81ea84b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}),\n",
       " HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, example=False),\n",
       " AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, example=False),\n",
       " HumanMessage(content='What is your name?', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "messages = template.format_messages(\n",
    "    name=\"Bob\",\n",
    "    user_input=\"What is your name?\"\n",
    ")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e473cb5c-34db-4210-8a49-c00e57d8ecde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\", additional_kwargs={}),\n",
       " HumanMessage(content='i dont like eating tasty things.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that re-writes the user's text to \"\n",
    "                \"sound more upbeat.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "template.format_messages(text='i dont like eating tasty things.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5d9ca-079a-437f-9a4e-af3875282d83",
   "metadata": {},
   "source": [
    "### Chains\n",
    "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components.\r\n",
    "\r\n",
    "LangChain provides the Chain interface for such \"chained\" applications. We define a Chain very generically as a sequence of calls to components, which can include other chain\n",
    "\n",
    "Chains allow us to combine multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. We can build more complex chains by combining multiple chains together, or by combining chains with other components.s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f907782-faee-44e5-895e-36cbdef40b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mTell me a funny joke about dog.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "Q: What did the dog say when he sat on sandpaper?\n",
      "A: Ruff!\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=joke_template, verbose=True)\n",
    "print(chain.run(adjective=\"funny\", content=\"dog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5a3b366-0002-4bc5-a1e5-8174236a9cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.6)\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
    ")\n",
    "name_chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"\"\"Suggest some menu items for {restaurant_name}\"\"\"\n",
    ")\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d74bfd6-3980-4fb4-b068-a27338048bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "- Chicken Tikka Masala\n",
      "- Saag Paneer\n",
      "- Lamb Vindaloo\n",
      "- Vegetable Samosas\n",
      "- Aloo Gobi\n",
      "- Tandoori Chicken\n",
      "- Naan Bread\n",
      "- Chana Masala\n",
      "- Gulab Jamun\n",
      "- Mango Lassi\n"
     ]
    }
   ],
   "source": [
    "chain = SimpleSequentialChain(chains = [name_chain, food_items_chain])\n",
    "\n",
    "content = chain.run(\"Indian\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91939162-63c7-4d41-810f-4d3046f5205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\")\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"Suggest some menu items for {restaurant_name}.\"\n",
    ")\n",
    "food_items_chain =LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b9d2b37-3ecf-4035-b002-9c261aef7eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = SequentialChain(\n",
    "    chains = [name_chain, food_items_chain],\n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = ['restaurant_name', \"menu_items\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d6f5fca-d4c5-4222-8593-3cf74cbc3cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Indian',\n",
       " 'restaurant_name': \" \\n\\nMaharaja's Palace Cuisine\",\n",
       " 'menu_items': '\\n\\n- Vegetable Samosas\\n- Palak Paneer\\n- Dal Makhani\\n- Tandoori Chicken\\n- Lamb Curry\\n- Chicken Tikka Masala\\n- Naan Bread\\n- Raita\\n- Basmati Rice\\n- Gulab Jamun\\n- Kulfi'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"cuisine\": \"Indian\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4090a330-3254-4d58-bf28-76886c595a3d",
   "metadata": {},
   "source": [
    "### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3aa0f296-883e-4aa0-8098-5730ef5d1f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find the GDP of the US for both years\n",
      "Action: Search\n",
      "Action Input: US GDP 2021 2022\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m$25.46 trillion\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to calculate the difference between the two years\n",
      "Action: Calculator\n",
      "Action Input: 25.46 trillion - 24.22 trillion\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 12400000000\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The difference in GDP between 2021 and 2022 was $1.24 trillion.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The difference in GDP between 2021 and 2022 was $1.24 trillion.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\"What was the difference GDP of US in 2021 & 2022?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce59468-7fcc-4d74-a6eb-c854469ecb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out when Elon Musk was born and then calculate his age in 2040.\n",
      "Action: Wikipedia\n",
      "Action Input: Elon Musk\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Elon Musk\n",
      "Summary: Elon Reeve Musk ( EE-lon; born June 28, 1971) is a business magnate and investor. Musk is the founder, chairman, CEO and chief technology officer of SpaceX;  angel investor, CEO, product architect and former chairman of Tesla, Inc.; owner, chairman and CTO of X Corp.; founder of the Boring Company; co-founder of Neuralink and OpenAI; and president of the Musk Foundation. He is the wealthiest person in the world, with an estimated net worth of US$226 billion as of September 2023, according to the Bloomberg Billionaires Index, and $249 billion according to Forbes, primarily from his ownership stakes in both Tesla and SpaceX.Musk was born in Pretoria, South Africa, and briefly attended the University of Pretoria before immigrating to Canada at age 18, acquiring citizenship through his Canadian-born mother. Two years later, he matriculated at Queen's University in Kingston, Ontario. Musk later transferred to the University of Pennsylvania, and received bachelor's degrees in economics and physics there. He moved to California in 1995 to attend Stanford University. However, Musk dropped out after two days and, with his brother Kimbal, co-founded online city guide software company Zip2. The startup was acquired by Compaq for $307 million in 1999, and with $12 million of the money he made, that same year Musk co-founded X.com, a direct bank. X.com merged with Confinity in 2000 to form PayPal. \n",
      "In 2002, eBay acquired PayPal for $1.5 billion, and that same year, with $100 million of the money he made, Musk founded SpaceX, a spaceflight services company. In 2004, he became an early investor in electric vehicle manufacturer Tesla Motors, Inc. (now Tesla, Inc.). He became its chairman and product architect, assuming the position of CEO in 2008. In 2006, Musk helped create SolarCity, a solar energy company that was acquired by Tesla in 2016 and became Tesla Energy. In 2013, he proposed a hyperloop high-speed vactrain transportation system. In 2015, he co-founded OpenAI, a nonprofit artificial intelligence research company. The following year, Musk co-founded Neuralink—a neurotechnology company developing brain–computer interfaces—and the Boring Company, a tunnel construction company. In 2022, he acquired Twitter for $44 billion and subsequently merged the company into newly created X Corp. and rebranded the service as X the following year. In March 2023, he founded xAI, an artificial-intelligence company.\n",
      "Musk has expressed views that have made him a polarizing figure. He has been criticized for making unscientific and misleading statements, including that of spreading COVID-19 misinformation, and promoting conspiracy theories. His Twitter ownership has been similarly controversial, including laying off a large number of employees, an increase in hate speech on the platform and features such as Twitter Blue and the implementation of limits on the amount of viewable Tweets per day being criticized. In 2018, the U.S. Securities and Exchange Commission (SEC) sued him for falsely tweeting that he had secured funding for a private takeover of Tesla. To settle the case, Musk stepped down as the chairman of Tesla and paid a $20 million fine.\n",
      "\n",
      "Page: Acquisition of Twitter by Elon Musk\n",
      "Summary: Business magnate Elon Musk initiated an acquisition of American social media company Twitter, Inc. on April 14, 2022, and concluded it on October 27, 2022. Musk had begun buying shares of the company in January 2022, becoming its largest shareholder by April with a 9.1 percent ownership stake. Twitter invited Musk to join its board of directors, an offer he initially accepted before declining. On April 14, Musk made an unsolicited offer to purchase the company, to which Twitter's board initially responded with a \"poison pill\" strategy to resist a hostile takeover, before unanimously accepting Musk's buyout offer of $44 billion on April 25. Musk stated that he planned to introduce new features to the platform, make its algorithms open-\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to calculate Elon Musk's age in 2040.\n",
      "Action: Calculator\n",
      "Action Input: (2040 - 1971)\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 69\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Elon Musk will be 69 years old in August 2040.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Elon Musk will be 69 years old in August 2040.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\"When was Elon musk born? What will be his age in August 2040?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21a9d28-e9dc-4188-a433-eecc738f79ed",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a31a06b-584d-4d44-86be-19be0c7d6993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Taco Palacio\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebc738a5-2d30-4346-a7d4-3360ffdb609d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"The Spice of India\"\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(\"Indian\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ec2d793-94cb-4af4-bb6f-1fb3adad49ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49601410-7d95-4526-babb-570049434acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Cielo Azul Mexican Cuisine\"\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff02efea-a197-44bc-9c88-f0c0ce74d2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Majestic Medina Cuisine\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(\"Arabic\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43d34163-04f8-413b-ba85-4f63ab88aa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Mexican\n",
      "AI: \n",
      "\n",
      "\"Cielo Azul Mexican Cuisine\"\n",
      "Human: Arabic\n",
      "AI: \n",
      "\n",
      "Majestic Medina Cuisine\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6d1442d-447b-4d54-b378-d20243148ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34e24216-d98a-4bd1-8e81-7e9d7d277976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The first cricket world cup was won by the West Indies in 1975. They defeated Australia by 17 runs in the final at Lord's Cricket Ground in London.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb006c9b-d83e-4962-be87-f4cbba3480cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 5+5 is equal to 10.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"How much is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c96701c8-378e-46a8-897e-78a6cee28b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The captain of the winning team in the 1975 Cricket World Cup was Clive Lloyd of the West Indies.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain ofthe winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7510283a-a158-4048-b165-cc185fba84d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Who won the first cricket world cup?\n",
      "AI:  The first cricket world cup was won by the West Indies in 1975. They defeated Australia by 17 runs in the final at Lord's Cricket Ground in London.\n",
      "Human: How much is 5+5?\n",
      "AI:  5+5 is equal to 10.\n",
      "Human: Who was the captain ofthe winning team?\n",
      "AI:  The captain of the winning team in the 1975 Cricket World Cup was Clive Lloyd of the West Indies.\n"
     ]
    }
   ],
   "source": [
    "print(convo.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "279bde46-7cc9-4c28-b4f1-008685adbb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first cricket world cup was won by the West Indies in 1975.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "convo = ConversationChain(\n",
    "    llm=OpenAI(temperature=0.7),\n",
    "    memory=memory\n",
    ")\n",
    "convo.run(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53460ffd-cd4d-47f8-89c7-527dc0ed3f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 10.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"How much is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c361c344-74a8-4157-970e-b04388191309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I'm sorry, I don't know.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain of the winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f36276f-a4d1-4d9b-a166-3b58f75d4c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Who was the captain of the winning team?\n",
      "AI:  I'm sorry, I don't know.\n"
     ]
    }
   ],
   "source": [
    "print(convo.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044f7f43-2f08-42ef-a541-e091048dafa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
